---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I obtained my Ph.D degree with honors from the University of Kansas in 2024. My research interest is efficient AI, to increase efficiency from all aspect including data efficiency, model efficiency and training/inference efficiency. I am actively looking for full-time job in related area. Please feel free to contact me if you have any opportunities available.


# üìù Research Path 

## Training/Finetuning Efficiency

- *Chen, X.*, et. al..[Slaying the HyDRA: Parameter-Efficient Hyper Networks with Low-Displacement Rank Adaptation](https://www.merl.com/publications/docs/TR2024-157.pdf). **NeurIPS Workshop 2024**
- *Chen, X.*, et. al..[SuperLoRA: Parameter-Efficient Unified Adaptation of Large Foundation Models](https://www.merl.com/publications/docs/TR2024-156.pdf). **BMVC 2024**
- *Chen, X.*, et. al..[SuperLoRA: Parameter-Efficient Unified Adaptation for Large Vision Models](https://www.merl.com/publications/docs/TR2024-062.pdf). **CVPR workshop (ECV) 2024**

## Model Efficiency
- *Chen, X.*, et. al. (2025). [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/pdf/2505.21835) arxiv 2025.
- Koike-Akino, T., *Chen, X.*, et. al. (2025). [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/pdf/2505.18413?) arxiv 2025.
- *Chen, X.*, et. al. (2023). [MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices](https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Chen_MOFA_A_Model_Simplification_Roadmap_for_Image_Restoration_on_Mobile_ICCVW_2023_paper.html). **ICCV workshop (RCV) 2023**.
- *Xiangyu Chen*, et. al. (2023). [Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets](https://arxiv.org/abs/2210.12333), **WACV 2023**.

## Data Efficiency
### Data Quality
- *Xiangyu Chen*, et. al. (2022). [Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets](https://arxiv.org/abs/2210.14319), **NeurIPS workshop (VTTA) 2022**.
- *Xiangyu Chen*, et. al. (2022). [Improving Vision Transformers on Small Datasets by Increasing Input Information Density in Frequency Domain](https://www.cs.ryerson.ca/~wangcs/papers/cvprw22.pdf), **extended abstract in CVPR Workshop (WiCV) 2022**.
### Data Quantity
- *Xiangyu Chen* et. al (2021). [Few-Shot Learning by Integrating Spatial and Frequency Representation](https://arxiv.org/pdf/2105.05348), **The Conference on Robots and Vision (CRV) 2021<span style="color:red"> *(oral)* </span>**

# üìñ Educations
- *2024*, Ph.D. in Electrical Engineering (with honors), University of Kansas, USA.
- *2018*, M.Eng., Southeast University, China.
- *2015*, B.Eng., Nanjing University of Science & Technology, China. 


# üíª Internships
- *2025.04 - present*, Computer Vision Research Intern, Sony, San Jose, CA, USA.
- *2024.07 - 2025.01*, Research Intern, [MERL](https://www.merl.com), Cambridge, MA, USA.
- *2023.11 - 2024.05*, Research Intern, [MERL](https://www.merl.com), Cambridge, MA, USA.
- *2023.01 - 2023.10*, Research Intern, SenseBrain, San Jose, CA, USA.
- *2018.07 - 2019.07*, Computer Vision Engineer intern, AIRIA, Nanjing, China.
- *2016.12 - 2017.02*, SDET intern, Meituan Dianping, Shanghai, China.

